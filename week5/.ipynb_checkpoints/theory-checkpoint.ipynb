{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Exercises\n",
    "Remember to read them all before class and try to solve them and figure out where you may have problems.\n",
    "Exercise 4 is the most important. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex: 1 Install and test Pytorch\n",
    "Later in the course we will be using the deep learning frame work pytorch. So install it. Install torchviz too to help plot computation graphs.\n",
    "\n",
    "Let us use automatic differentation for another gradient descent algorithm.\n",
    "\n",
    "Lets do a similar exercise to last time just with some data and Linear Regression Gradient Descent.\n",
    "First we need to understand that to represent data we must use torch tensors. Tensors are very much like numpy arrays just with some extra functionality.\n",
    "The thing we will consider is the backward function that computes gradients of whatever computation you have made using torch tensors.\n",
    "\n",
    "To see how this works, lets see an example.\n",
    "Lets evaluate the gradient of the sigmoid function without actually knowing the formula. All you need to know is how to compute the function using standard functions i.e. $s(z) = 1/(1+e^{-z})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of f(z)=1/(1 + e^{-z}) at z = 0 tensor([0.2500])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "z = torch.zeros(1, requires_grad=True)\n",
    "sz = 1.0 / (1+ torch.exp(-z))\n",
    "sz.backward() # compute gradient of sz relative to z in this case\n",
    "print('Gradient of f(z)=1/(1 + e^{-z}) at z = 0', z.grad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2: Gradient Descent with pytorch (manually)\n",
    "In deep learning frameworks all we usually need to do is to show how to compute the cost in a given point and the system then automatically computes the gradient in that data point for you. We will se how later in this course, in this exercise we will try and see if we can use this functionality to do gradient descent.\n",
    "The example will be similar to last weeks gradient descent except now we actually make a data set to run gradient descent on for Linear Regression.\n",
    "\n",
    "\n",
    "**Setup:**\n",
    "We create a data set $D$ that will consist of $n=100$ data points in 2D $(x_1,x_2)$ i.e. two features $x_1, x_2$. \n",
    "The data feature vectors of $x_1$ and $x_2$ are made orthogonal and $x_1$ has unit norm while $x_2$ has norm $a$.\n",
    "\n",
    "We generate a target vector \n",
    "$$\n",
    "y = x_1+x_2\n",
    "$$ \n",
    "which is also a vector of length $n$ i.e. the data we are trying to fit comes from *(a very simple)* linear model.\n",
    "\n",
    "Remember linear regression the in sample error/cost is \n",
    "$$\n",
    "\\textrm{E}_\\textrm{in}(w) = \\frac{1}{n} \\sum_{i=1}^n (w^\\intercal x_i - y_i) = \\frac{1}{n} \\|Xw -y\\|^2\n",
    "$$\n",
    "\n",
    "\n",
    "We have written the code to generate data and the surrounding Gradient Descent for loop, all you need is to write the code for computing the cost (ein).\n",
    "You can only use commands from torch here (no numpy), but you can use standard operators like $+,-,*,/,**$ on torch tensors that work like their numpy equivalent and torch.sum may be very handy\n",
    "**Complete the gradient descent code below by computing cost using standard operations and torch commands only**\n",
    "\n",
    "The gradient descent will start the search at $w=(42, 2)$ for some reason. We have also sat an almost arbitrary learning rate. You can change both if you like.\n",
    "\n",
    "\n",
    "To see how this linear regression exercise relates to the gradient descent exercie from last week try increasing the value of $a$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: Cost 1730.0000000000002\n",
      "epoch 1: Cost 332704.99999999977\n",
      "epoch 2: Cost 1908984305.0000012\n",
      "epoch 3: Cost 11911292205905.006\n",
      "epoch 4: Cost 7.43383317955875e+16\n",
      "epoch 5: Cost 4.6394552805047776e+20\n",
      "epoch 6: Cost 2.8954840405520577e+24\n",
      "epoch 7: Cost 1.8070715897085218e+28\n",
      "epoch 8: Cost 1.1277933791370877e+32\n",
      "epoch 9: Cost 7.038558479194564e+35\n",
      "epoch 10: Cost 4.3927643468653243e+39\n",
      "epoch 11: Cost 2.741524228878648e+43\n",
      "epoch 12: Cost 1.7109852712431657e+47\n",
      "epoch 13: Cost 1.0678259077828593e+51\n",
      "epoch 14: Cost 6.664301490472831e+54\n",
      "epoch 15: Cost 4.159190560204095e+58\n",
      "epoch 16: Cost 2.595750828623375e+62\n",
      "epoch 17: Cost 1.6200080921438473e+66\n",
      "epoch 18: Cost 1.0110470503069752e+70\n",
      "epoch 19: Cost 6.309944640965825e+73\n",
      "epoch 20: Cost 3.9380364504267704e+77\n",
      "epoch 21: Cost 2.4577285487113475e+81\n",
      "epoch 22: Cost 1.5338683872507506e+85\n",
      "epoch 23: Cost 9.572872604831931e+88\n",
      "epoch 24: Cost 5.9744297926756e+92\n",
      "epoch 25: Cost 3.72864163360884e+96\n",
      "epoch 26: Cost 2.3270452435352765e+100\n",
      "epoch 27: Cost 1.4523089364903654e+104\n",
      "epoch 28: Cost 9.06386007263637e+107\n",
      "epoch 29: Cost 5.656755071332362e+111\n",
      "epoch 30: Cost 3.530380840018523e+115\n",
      "epoch 31: Cost 2.2033106822555595e+119\n",
      "epoch 32: Cost 1.3750861967956938e+123\n",
      "epoch 33: Cost 8.581912954201928e+126\n",
      "epoch 34: Cost 5.355971874717424e+130\n",
      "epoch 35: Cost 3.3426620470111424e+134\n",
      "epoch 36: Cost 2.0861553835396507e+138\n",
      "epoch 37: Cost 1.3019695748670944e+142\n",
      "epoch 38: Cost 8.125592116745525e+145\n",
      "epoch 39: Cost 5.071182040060883e+149\n",
      "epoch 40: Cost 3.1649247112019986e+153\n",
      "epoch 41: Cost 1.9752295122611678e+157\n",
      "best w found tensor([                -311874458854200981825278883611357825477996889592172239449489408.,\n",
      "        87776019596833545128073761588123056156297639214341487748969581671185098160996352.],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch # the torch tensor library\n",
    "\n",
    "# CREATE SOME DATA\n",
    "n = 100\n",
    "x1 = np.random.rand(n)\n",
    "x2 = np.random.rand(n)\n",
    "# Grahm schmidt process\n",
    "x1 = x1/np.linalg.norm(x1)\n",
    "x2 = x2/np.linalg.norm(x2)\n",
    "x2 = x2 - np.dot(x1, x2) * x1 #\n",
    "x2 = x2/np.linalg.norm(x2)\n",
    "\n",
    "# CREATE THE DATA MATRIX\n",
    "a = 4.0\n",
    "D = np.c_[x1, a*x2]\n",
    "# CREATE TARGET FUNCTION VECTOR\n",
    "y = x1 + x2\n",
    "\n",
    "# MAKE TORCH VARIABLES TO USE\n",
    "X = torch.from_numpy(D).double()\n",
    "ty = torch.from_numpy(y).double()\n",
    "ni = torch.tensor(1./n, dtype=torch.double)\n",
    "\n",
    "def torch_gd():\n",
    "    w = torch.tensor([42.0, 2.0], dtype=torch.double)\n",
    "    lr = torch.tensor(10.0/a, dtype=torch.double)\n",
    "    epochs = 42\n",
    "    cost_hist = []\n",
    "    for i in range(epochs):\n",
    "        w.requires_grad_() # say i want gradient relative to w in upcoming computation\n",
    "        cost = None\n",
    "        ### YOUR CODE HERE - Compute Ein for linear regression as function of w and store in cost 1 - 4 lines\n",
    "        #cost = ((1/n) * torch.norm((X @ w - ty)**2))\n",
    "        #cost = torch.sum( ( X@ w - ty) ** 2) / n\n",
    "        cost = torch.sum( ( X@ w - ty) ** 2).mean()\n",
    "        ### END CODE\n",
    "        cost_hist.append(cost)\n",
    "        print('epoch {0}: Cost {1}'.format(i, cost))\n",
    "        cost.backward() # compute gradient cost as function of w\n",
    "        w = w - lr * w.grad # update w\n",
    "        w.detach_() # removes w from current computation graph\n",
    "    print('best w found', w)\n",
    "torch_gd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3:  Bias Variance \n",
    "\n",
    "-   Does Bias and Variance terms (two numbers) in the Bias Variance\n",
    "    decomposition depend on the learning algorithm.\n",
    "    \n",
    "        - Yes. \n",
    "        - Variancen = kun en hypetese = så har du ingen variane.\n",
    "\n",
    "-   What is Variance (in Bias Variance tradoff) if we have a hypothesis\n",
    "    set of size $1$ namely the constant model $h(x) = 2$. The learning\n",
    "    algorithm always picks this hypothesis no matter the data.\n",
    "    \n",
    "        - 0 \n",
    "        - Var [x] = E[ g(x) - \\hat g(x))^2 ]\n",
    "\n",
    "-   What is the Variance (in the Bias Variance tradeoff) if the simple\n",
    "    hypothesis from the previous question is replaced by a very very\n",
    "    sophisticated hypothesis.\n",
    "    \n",
    "        - Same.\n",
    "\n",
    "-   Assume the target function is a second degree polynomial, and the\n",
    "    input to your algorithm is always eleven (noiseless) points. Your\n",
    "    hypothesis set is the set of all degreee 10 polynomial and the\n",
    "    learning algorithm returns the hypothesis with the best fit\n",
    "    (miniming least squared error) given the data. What is Bias and what\n",
    "    is Variance?\n",
    "    \n",
    "        - Bias[x] = \\hat g(x) - f(x) \n",
    "        - Bias vil være 0 \n",
    "        - Vi kan kun fitte target function. Så får vi Var(x) = 0. KUN FORDI NOISELESS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4: Bias Variance Experiment \n",
    "In this exercise you must redo the experiment shown at the lectures.\n",
    "This exercise takes up quite a lot of space so we have moved it to a separate notebook. Go to [BiasVariance Notebook](BiasVariance.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 5: Bias Variance - Hard Exercise\n",
    "Book Problem 2.24 part (a)\n",
    "\n",
    "Short Version:\n",
    "   \n",
    "  - The target function is $f(x) = x^2$ and the cost is Least Squares.\n",
    "\n",
    "  - Sample two points $x_1, x_2$ from $[-1, 1]$ uniformly at random to get the data set $D = \\{(x_1, x_1^2), (x_2, x_2^2)\\}$\n",
    "\n",
    "  - Use hypothesis space $\\{h(x) = ax +b\\mid a,b\\in\\Bbb R\\}$ i.e. lines. There are two parameters $a$ and $b$.\n",
    "\n",
    "  - Given a data set $D = \\{(x_1, x_1^2), (x_2, x_2^2)\\}$ the algorithm returns the line that fits these points.\n",
    "\n",
    "  - Your task is to write down an analytical expression for $\\bar{g} = \\mathbb{E}_D [h_D]$ where $h_D$ is the hypothesis learned on D.\n",
    "\n",
    "**Step 1.** What is the in sample error of $h_D$ and why?\n",
    "\n",
    "**Step 2.** Given $D$ what are $a, b$ (defined by the line between $(x_1, x_1^2)$ and  $(x_2, x_2^2)$)? Hint: $x_2^2- x_1^2 = (x_2-x_1)(x_2 + x_1)$.\n",
    "\n",
    "**Step 3.** What is the expected value of the slope $a$ over $x_1$ and $x_2$?\n",
    "\n",
    "**Step 4.** What is the expected value of the intercerpt $b$ over $x_1$ and $x_2$? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**More hints**\n",
    "For the uniform distribution over $[-1,1]$ the mean is $0$ and the variance is $\\frac{1}{2}\\int_{-1}^1 (x-0)^2 dx = \\frac{1}{3}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 6: Regularization with Weight decay\n",
    "If we use weight decay regularization ($\\lambda||w||^2)$  for some real number $\\lambda$ in Linear Regression what \n",
    "happens to the optimal weight vector if we let $\\lambda \\rightarrow \\infty$? (cost is $\\frac{1}{n} \\|Xw - y\\|^2 + \\lambda \\|w\\|^2$)\n",
    "\n",
    "What if we set $\\lambda = -7?$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 7: Grid Search For Regularization and Validation - Sklearn\n",
    "In this exercise you must we will optimize a [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) using regularization and validation.\n",
    "You must use the in grid search module [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) from sklearn.\n",
    "\n",
    "In the cell below we have shown an example of how to use the gridsearch module to find a good height decision tree for Decision Trees.\n",
    "\n",
    "Your job is to find a good hyperparameters for decision trees for the industri codes data set!\n",
    "\n",
    "### Task 1:\n",
    "For the industri code text classification data set (See week 2 for more info) find the best combination of max_depth and  min_samples_split  (cell two below)\n",
    "\n",
    "The **max_depth** parameter controls the max depth of a tree and the deeper the tree the more complex the model.\n",
    "\n",
    "The **min_samples_split** controls how many elements the algorithm that constructs the tree is allowed to try and split.\n",
    "So if a subtree contains less than min_leaf_size elements it many not be split into a larger subtree by the algorithm.\n",
    "\n",
    "\n",
    "**Hint:** For the wine data set it is enough to test numbers between 1 and 20 for max_depth, results may differ due to randomness in data split.\n",
    "\n",
    "### Task 2:\n",
    "- How important does the max_depth value seem to be for Decision Trees for Wine classification?\n",
    "- How important is max_depth and min_leaf_size for industri codes classification with Trees?\n",
    "- How long time does it take to use grid search validation for $k$ hyperparamers where we test each parameter for $d$ values?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 7}</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.904494</td>\n",
       "      <td>0.051327</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>11</td>\n",
       "      <td>{'max_depth': 11}</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.904494</td>\n",
       "      <td>0.051327</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 3}</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>0.068726</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.997222</td>\n",
       "      <td>0.003928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 5}</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.848315</td>\n",
       "      <td>0.061062</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>9</td>\n",
       "      <td>{'max_depth': 9}</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.848315</td>\n",
       "      <td>0.061062</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 1}</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.062196</td>\n",
       "      <td>6</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.669492</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.679708</td>\n",
       "      <td>0.009136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "3       0.000515      0.000034         0.000174        0.000002   \n",
       "5       0.000469      0.000046         0.000166        0.000006   \n",
       "1       0.000453      0.000027         0.000166        0.000002   \n",
       "2       0.000481      0.000071         0.000167        0.000003   \n",
       "4       0.000539      0.000040         0.000201        0.000033   \n",
       "0       0.000327      0.000030         0.000174        0.000016   \n",
       "\n",
       "  param_max_depth             params  split0_test_score  split1_test_score  \\\n",
       "3               7   {'max_depth': 7}           0.950000           0.833333   \n",
       "5              11  {'max_depth': 11}           0.950000           0.833333   \n",
       "1               3   {'max_depth': 3}           0.783333           0.833333   \n",
       "2               5   {'max_depth': 5}           0.783333           0.833333   \n",
       "4               9   {'max_depth': 9}           0.783333           0.833333   \n",
       "0               1   {'max_depth': 1}           0.566667           0.583333   \n",
       "\n",
       "   split2_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "3           0.931034         0.904494        0.051327                1   \n",
       "5           0.931034         0.904494        0.051327                1   \n",
       "1           0.948276         0.853933        0.068726                3   \n",
       "2           0.931034         0.848315        0.061062                4   \n",
       "4           0.931034         0.848315        0.061062                4   \n",
       "0           0.706897         0.617978        0.062196                6   \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "3            1.000000            1.000000            1.000000   \n",
       "5            1.000000            1.000000            1.000000   \n",
       "1            1.000000            1.000000            0.991667   \n",
       "2            1.000000            1.000000            1.000000   \n",
       "4            1.000000            1.000000            1.000000   \n",
       "0            0.677966            0.669492            0.691667   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "3          1.000000         0.000000  \n",
       "5          1.000000         0.000000  \n",
       "1          0.997222         0.003928  \n",
       "2          1.000000         0.000000  \n",
       "4          1.000000         0.000000  \n",
       "0          0.679708         0.009136  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter found {'max_depth': 7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image, display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def show_result(clf):\n",
    "    df = pd.DataFrame(clf.cv_results_)\n",
    "    df = df.sort_values('mean_test_score', ascending=False)\n",
    "    display(df)\n",
    "    print('best parameter found', clf.best_params_)\n",
    "    best_tree = DecisionTreeClassifier(**clf.best_params_)\n",
    "    best_tree.fit(X, y)\n",
    "    return best_tree\n",
    "    \n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "# grid search validation\n",
    "reg_parameters = {'max_depth': list(range(1, 12, 2))}  # dict with all parameters we need to test\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), reg_parameters, cv=3, return_train_score=True)\n",
    "clf.fit(X, y)\n",
    "# code for showing the result\n",
    "bt = show_result(clf)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def load_branche_data(keys):\n",
    "    \"\"\"\n",
    "    Load the data in branche_data.npz and save it in lists of\n",
    "    strings and labels (whose entries are in {0,1,..,num_classes-1})\n",
    "    \"\"\"\n",
    "    filename = 'branchekoder_formal.gzip'\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, 'wb') as fh:\n",
    "            path = \"http://users-cs.au.dk/jallan/ml/data/{0}\".format(filename)\n",
    "            fh.write(urllib.request.urlopen(path).read())\n",
    "    data = pd.read_csv(filename, compression='gzip')\n",
    "    actual_class_names = []\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i, kv in enumerate(keys):\n",
    "        key = kv[0]\n",
    "        name = kv[1]\n",
    "        strings = data[data.branchekode == key].formal.values\n",
    "        features.extend(list(strings))\n",
    "        label = [i] * len(strings)\n",
    "        labels.extend(label)\n",
    "        actual_class_names.append(name)\n",
    "    assert len(features) == len(labels)\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    return features, labels, actual_class_names\n",
    "\n",
    "\n",
    "def get_branche_data(keys):\n",
    "    features, labels, actual_class_names = load_branche_data(keys)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "    return X_train, X_test, y_train, y_test, actual_class_names\n",
    "\n",
    "\n",
    "keys = [(561010, 'Restauranter'), (620100, 'Computerprogrammering')]\n",
    "feat_train, feat_test, lab_train, lab_test, cnames = get_branche_data(keys)\n",
    "\n",
    "def decisiontree_model_selection(feat_train, feat_test):\n",
    "    c = CountVectorizer()\n",
    "    c.fit(feat_train)\n",
    "    bag_of_words_feat_train = c.transform(feat_train)\n",
    "    bag_of_words_feat_test = c.transform(feat_test)\n",
    "    clf = None\n",
    "    ### YOUR CODE HERE\n",
    "    ### END CODE\n",
    "    return clf\n",
    "###\n",
    "clf = decisiontree_model_selection(feat_train, feat_test)\n",
    "bt = show_result(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 8: The Wrong and Right Way to Do (Cross) Validation\n",
    "In this exercise we will try and see a way of using cross validation for learning with $d$ features $N$ data points with $d>>N$.\n",
    "The idea would to do something as follows.\n",
    "\n",
    "- Step 1: Screen predictors: \n",
    "    Select predictors that correlate best with target (find best features)\n",
    "\n",
    "- Step 2. Simplify input: \n",
    "\tBuild classifier using  good predictors only \n",
    "\n",
    "- Step 3. Use cross-validation:\n",
    "\t Estimate tuning parameters and Eout (there will be no parameter tuning in our experiment - just measure eout)\n",
    "     \n",
    "### The Experiment - Cross Validation for Binary Classification\n",
    "<!--Given $D=\\{(x_1, y_1),...,(x_{50}, y_{50})\\}$ where $x_i\\in \\mathbb{R}^{5000}$ and $y_i\\in \\{0, 1\\}$. Let us assume 25 $y_i$'s are $0$'s and the remainding $25$ $y_i$'s are $1$'s. Furthermore, each entry of $x_i$ are sampled from a standard Gaussian independent of the target\n",
    "\n",
    "It might be difficult to handle $D$ because $n=50<<d=5000$, the number of points is much smaller than each points dimension! To handle this, -->\n",
    "\n",
    "\n",
    "**Setup:**\n",
    "- Input: 5000 features, 50 data points. In other words let $D=\\{(x_1, y_1),...,(x_{50}, y_{50})\\}$ where $x_i\\in \\mathbb{R}^{5000}$ and $y_i\\in \\{0, 1\\}$. \n",
    "- Let 25 points have $y_i=1$ and the remainding 25 points have $y_i=0$.  \n",
    "- Each feature is sampled from standard Gaussian independent of target. \n",
    "\n",
    "Notice that the number of points is much smaller than each points dimension, that is, $n=50<<d=5000$. \n",
    "\n",
    "**Learning Algorithm:**\n",
    "- Find the 100 features [correlating](https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html) highest with target data (they are probably the most usefull and  5000 features for 50 data points seems to many) \n",
    "\n",
    "- Use the <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#The_1-nearest_neighbour_classifier\">1-Nearest Neighbor Classifier</a>:<br> \n",
    "We have not covered that but is is really easy to understand what it does. \n",
    "Given test data $x$ the algorithm finds the nearest point (under euclidian norm or some other predefined choice) in the dataset and returns that label. So learning algorithm just stores the data set for later use somehow!.\n",
    "- Estimate out of sample error with cross validation. Use 40 points to do Nearest Neighbor Classification and evaluate the performance on the remainding 10 points.   \n",
    "\n",
    "This gives CV error around 0-6%. Is this correct? Why, why not.\n",
    "\n",
    "You can see the experiment implemented in python below. \n",
    "\n",
    "**Task:** If you think there is something wrong you can try and make a new better experiment that shows the right way to do Cross Validation and then show experimentally what the error should be in this scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# If you get \"ImportError\" on this line you probably have an old version of sklearn < 0.18\n",
    "# (On linux you can update by 'pip uninstall scikit-learn' then 'pip install scikit-learn')\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "\n",
    "def run_exp():\n",
    "    nr_feat = 5000\n",
    "    nr_points = 50\n",
    "    dat = np.random.randn(nr_points, nr_feat)\n",
    "    \n",
    "    cs = int(nr_points/2)\n",
    "    target = np.ones(nr_points)\n",
    "    target[:cs] = 0\n",
    "    np.random.shuffle(target)\n",
    "    #print('target sum',target.sum())\n",
    "    cor = np.array([np.corrcoef(dat[:, i], target)[0,1] for i in range(nr_feat)])\n",
    "\n",
    "    idx = np.argsort(cor)[::-1]\n",
    "    #print('cor of first features',cor[idx[0:100]])\n",
    "    feat_to_use = dat[:,idx[0:100]]\n",
    "    X = feat_to_use\n",
    "    y = target\n",
    "    print('mean correlations in top 100 data', np.mean(cor[idx[0:100]]))\n",
    "    print('mean correlations in all data', np.mean(cor))\n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    cv_errors = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        cor10 = np.array([np.corrcoef(X_test[:, i], y_test)[0,1] for i in range(100)])\n",
    "        print('cor between X_test and y_test ', cor10.mean())\n",
    "        nn_classifier = KNN(n_neighbors=1, algorithm='brute').fit(X_train, y_train)\n",
    "        nn_predictions = nn_classifier.predict(X_test)\n",
    "        error_prob = (nn_predictions != y_test).mean()\n",
    "        print('\\nSplit {0}:'.format(i))\n",
    "        print('Nearest Neighbour Cross Validation Error:', error_prob)\n",
    "        acc = 1-error_prob\n",
    "        print('Nearest Neighbour Cross Validation Accuracy:', acc)        \n",
    "        cv_errors.append(error_prob)\n",
    "    \n",
    "    print('*'*10)\n",
    "    print('Average Cross Validation Error {0}'.format(np.mean(np.array(cv_errors))))\n",
    "\n",
    "run_exp()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
